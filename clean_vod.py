import requests
import json
import concurrent.futures

# é…ç½®åŒº
SOURCE_URL = "https://raw.githubusercontent.com/gaotianliuyun/gao/master/js.json"
EXCLUDE_KEYWORDS = ["ç½‘ç›˜", "é˜¿é‡Œ", "å¤¸å…‹", "UC", "PikPak", "æœç´¢", "äº‘ç›˜", "ç›˜", "115"]

def check_url(site):
    """æµ‹è¯•æ¥å£è¿é€šæ€§"""
    name = site.get("name", "æœªçŸ¥")
    api = site.get("api", "")
    
    # å¦‚æœä¸æ˜¯ http å¼€å¤´çš„ï¼Œé€šå¸¸æ˜¯åŠ å¯†æ¥å£æˆ–å†…éƒ¨åè®®ï¼Œæš‚æ—¶ä¿ç•™
    if not api.startswith("http"):
        return site

    try:
        # å°è¯•è¯·æ±‚æ¥å£ï¼Œè¶…æ—¶è®¾ä¸º 3 ç§’
        resp = requests.head(api, timeout=3, allow_redirects=True)
        if resp.status_code < 400:
            return site
    except:
        pass
    
    print(f"[-] å‰”é™¤å¤±æ•ˆæº: {name}")
    return None

def main():
    try:
        print("ğŸš€ æ­£åœ¨ä¸‹è½½åŸå§‹æº...")
        resp = requests.get(SOURCE_URL, timeout=15)
        data = resp.json()

        # 1. å½»åº•æ¸…ç©ºç›´æ’­
        data["lives"] = []

        # 2. é¢„è¿‡æ»¤ï¼šæ ¹æ®å…³é”®è¯å‰”é™¤ç½‘ç›˜/æœç´¢ç±»
        original_sites = data.get("sites", [])
        filtered_sites = [
            s for s in original_sites 
            if not any(k in s.get("name", "") for k in EXCLUDE_KEYWORDS)
        ]
        print(f"å…³é”®è¯è¿‡æ»¤å®Œæˆï¼Œå‰©ä½™ {len(filtered_sites)} ä¸ªï¼Œå‡†å¤‡æµ‹è¯•è¿é€šæ€§...")

        # 3. å¹¶å‘æµ‹è¯•è¿é€šæ€§ (å¼€å¯ 10 ä¸ªçº¿ç¨‹)
        final_sites = []
        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
            results = list(executor.map(check_url, filtered_sites))
            final_sites = [r for r in results if r is not None]

        data["sites"] = final_sites
        
        # 4. ä¿å­˜ç»“æœ
        with open("my_vod.json", "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
            
        print(f"âœ… å¤„ç†å®Œæˆï¼æœ€ç»ˆä¿ç•™ç‚¹æ’­æº: {len(final_sites)} ä¸ª")

    except Exception as e:
        print(f"âŒ è¿è¡Œå‡ºé”™: {e}")

if __name__ == "__main__":
    main()
